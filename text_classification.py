# -*- coding: utf-8 -*-
"""Text_Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BmuiGFZeLSf2SJGpIFQ7P_NyD1Slj0KR
"""

from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

stop_words=stopwords.words('english')

!pip install spacy
!python -m spacy download en_core_web_md

class Catogary:
  BOOKS="book"
  CLOTHING="Clothing"

train_x=["i love the book", "this is a great book","the fit is great","i love the shoes"]
train_y=[Catogary.BOOKS,Catogary.BOOKS,Catogary.CLOTHING,Catogary.CLOTHING]



from sklearn.feature_extraction.text import CountVectorizer

vectorizor=CountVectorizer(binary=True);
vector_train_x=vectorizor.fit_transform(train_x);

from sklearn import svm
clf_svm=svm.SVC(kernel='linear')
clf_svm.fit(vector_train_x,train_y)
vector_train_x=vectorizor.transform(["i love the books"])
clf_svm.predict(vector_train_x)

import spacy

nlp=spacy.load('en_core_web_md')

doc=[nlp(text) for text in train_x]

doc[0]

print(doc)

print(doc[0].vector)

train_x_word_vector=[text.vector  for text in doc]
print(train_x_word_vector)

from sklearn import svm
clf_svm_wv=svm.SVC(kernel='linear')
clf_svm_wv.fit(train_x_word_vector,train_y)
test_x=["i love the hat"]
test_doc=[nlp(text) for text in test_x]
clf_svm_wv.predict([x.vector for x in test_doc])

import nltk

nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('punkt')

from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
stemmer=PorterStemmer()
phrase ='reading the books.'
words=word_tokenize(phrase)
stammed_word=[]
for word in words:
  stammed_word.append(stemmer.stem(word))
for w in stammed_word:
  print(w)

" ".join(stammed_word)

from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
lemmatizer=WordNetLemmatizer()

phrase='reading the books'
words=word_tokenize(phrase)

lammtize_words=[]
for word in words:
  lammtize_words.append(lemmatizer.lemmatize(word, pos='v'))

" ".join(lammtize_words)

from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
stop_words=stopwords.words('english')

phrase='Here is an example sentence demonstrating the removal of stopwords'
words=word_tokenize(phrase)

stripped_phrase=[]

for word in words:
  if word not in stop_words:
    stripped_phrase.append(word)

print(" ".join(stripped_phrase))

!python -m textblob.download_corpora

#Textblob
from textblob import TextBlob

phrase='the book thst i love'

tb_phrase=TextBlob(phrase)

tb_phrase.correct()

tb_phrase.tags

tb_phrase.sentiment

